{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f91033-ae53-4016-9e4c-1eda3cdea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextFind demo on importing pdf documents and doing semantic searches and question answering\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6bf9961-1bc3-4897-9ccb-5bcc566bccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic class used by all the extractors to accomodate the extracted text for saving into TextFind\n",
    "\n",
    "import abc\n",
    "\n",
    "\n",
    "class BaseExtractor:\n",
    "    def __init__(self, text_chunk_max_size):\n",
    "        self.text_chunk_max_size = text_chunk_max_size\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def page_text_provider(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def extract_text(self, text_provider):\n",
    "        pass\n",
    "\n",
    "    def extract_text_chunks(self):\n",
    "        text_chunks_arr = []\n",
    "        current_text_chunk = ''\n",
    "        for text_provider in self.page_text_provider():\n",
    "            text_gen = self.extract_text(text_provider)\n",
    "            current_text_chunk += text_gen\n",
    "            if len(current_text_chunk) > self.text_chunk_max_size:\n",
    "                text_chunks_arr.append(current_text_chunk)\n",
    "                current_text_chunk = ''\n",
    "            else:\n",
    "                current_text_chunk += text_gen\n",
    "        if len(current_text_chunk) > 0:\n",
    "            text_chunks_arr.append(current_text_chunk)\n",
    "        return text_chunks_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62603970-8d1e-4ea5-819f-9c42d85b6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Pdf text extractor. Not that fancy but it does its job\n",
    "\n",
    "from pdfminer.layout import LTFigure\n",
    "from pdfminer.layout import LTTextBox\n",
    "from pdfminer.layout import LTChar\n",
    "from pdfminer.layout import LTTextLine\n",
    "from pdfminer.high_level import extract_pages\n",
    "\n",
    "\n",
    "class PdfExtractor(BaseExtractor):\n",
    "\n",
    "    def page_text_provider(self):\n",
    "        return extract_pages(self.pdf_file)\n",
    "\n",
    "    def extract_text(self, layout):\n",
    "        layout_text = ''\n",
    "        for lobj in layout:\n",
    "            layout_text += PdfExtractor.__text(lobj)\n",
    "        return layout_text\n",
    "\n",
    "    def __init__(self, pdf_file, text_chunk_max_size):\n",
    "        BaseExtractor.__init__(self, text_chunk_max_size=text_chunk_max_size)\n",
    "        self.pdf_file = pdf_file\n",
    "\n",
    "    def page_text_provider(self):\n",
    "        return extract_pages(self.pdf_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def __text(lobj):\n",
    "        lobj_text = ''\n",
    "        if isinstance(lobj, LTTextBox):\n",
    "            for element in lobj:\n",
    "                if isinstance(element, LTTextLine):\n",
    "                    lobj_text = lobj_text + element.get_text()\n",
    "        elif isinstance(lobj, LTFigure):\n",
    "            for element in lobj:\n",
    "                if isinstance(element, LTChar):\n",
    "                    lobj_text = lobj_text + element.get_text()\n",
    "        return lobj_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dc4d9b7-c8a1-4039-9aaa-aa5dca94a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtExtractor(BaseExtractor):\n",
    "\n",
    "    def __init__(self, txt_file, text_chunk_max_size):\n",
    "        BaseExtractor.__init__(self, text_chunk_max_size=text_chunk_max_size)\n",
    "        self.txt_file = txt_file\n",
    "        self.file = None\n",
    "\n",
    "    def page_text_provider(self):\n",
    "        try:\n",
    "            if self.file is None:\n",
    "                self.file = open(self.txt_file)\n",
    "            while True:\n",
    "                text = self.file.read(self.text_chunk_max_size)\n",
    "                if not text:\n",
    "                    break\n",
    "                yield text\n",
    "        finally:\n",
    "            self.file.close()\n",
    "\n",
    "    def extract_text(self, text):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "541341d4-b3a5-4907-8dc8-d9dfe0a65d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utitlity method to reject lines smaller than the min_line_length value.\n",
    "# and to remove some newlines (prone to changes, but for now it does the work)\n",
    "\n",
    "def text_chunk_arr_to_sentences(text_chunk_arr, min_line_length=10):\n",
    "    sentences = []\n",
    "    for text_chunk in text_chunk_arr:\n",
    "        lines = text_chunk.splitlines()\n",
    "        sentence = ''\n",
    "        for line in lines:\n",
    "            if line.endswith('.'):\n",
    "                sentence += line\n",
    "                sentences.append(sentence)\n",
    "                sentence = ''\n",
    "            else:\n",
    "                if len(line) >= min_line_length:\n",
    "                    sentence += line\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df59f4c8-94c8-485b-94a6-a53ab809846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a minimal TextFind client to exercise a few TextFind endpoints like:\n",
    "# creating documents, uploading files, querying some basic metadata (channel and channel properties)\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import mimetypes\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TextFindClient:\n",
    "\n",
    "    API_KEY = \"api-key\"\n",
    "\n",
    "    FILE_DOCUMENT_CHANNEL_NAME = \"__file_documents__\"\n",
    "\n",
    "    FILE_PAGE_DOCUMENTS_CHANNEL_NAME = \"__file_page_documents__\"\n",
    "\n",
    "    def __init__(self, url, app_key=None, verify_ssl_cert=True):\n",
    "        self.app_key = app_key\n",
    "        self.url = url\n",
    "        self.verify_ssl_cert = verify_ssl_cert\n",
    "\n",
    "    def create_document(self, topic_uuid, channel_uuid, document, shared_with_group_uuids=[], file_object_ids=[]):\n",
    "        document_context = copy.deepcopy(document)\n",
    "        if len(file_object_ids) > 0:\n",
    "            document_context[\"relatedFileObjectIDs\"] = ','.join(file_object_ids)\n",
    "        else:\n",
    "            document_context[\"relatedFileObjectIDs\"] = None\n",
    "        document = dict()\n",
    "        document['topicUuid'] = topic_uuid\n",
    "        document['channelUuid'] = channel_uuid\n",
    "        document['sharedWithGroupUuids'] = shared_with_group_uuids\n",
    "        document['context'] = document_context\n",
    "        response = requests.post(url=f'{self.url}/sas-api/documents/text-documents',\n",
    "                                 json=document, headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        if not response.ok:\n",
    "            logging.error(response.text)\n",
    "            raise Exception(response.text)\n",
    "\n",
    "    @staticmethod\n",
    "    def __mimetype(file_path):\n",
    "        return mimetypes.guess_type(file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def __basename(file_path):\n",
    "        return os.path.basename(file_path)\n",
    "\n",
    "    def upload_file(self, file_path):\n",
    "        basename = TextFindClient.__basename(file_path)\n",
    "        mt = TextFindClient.__mimetype(file_path)\n",
    "\n",
    "        files = {'file': (basename, open(file_path, 'rb'), mt)}\n",
    "        response = requests.post(f'{self.url}/sas-api/file-manager/file',\n",
    "                                 files=files,\n",
    "                                 headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        if not response.ok:\n",
    "            logging.error(response.text)\n",
    "            raise Exception(response.text)\n",
    "        else:\n",
    "            is_success = json.loads(response.text)[\"success\"]\n",
    "            if is_success:\n",
    "                return json.loads(response.text)[\"fileObjectId\"]\n",
    "            else:\n",
    "                logging.debug(json.loads(response.text)[\"errorMessage\"])\n",
    "                raise Exception(json.loads(response.text)[\"errorMessage\"])\n",
    "\n",
    "    def create_document_from_file(self,\n",
    "                                  file_path,\n",
    "                                  topic_uuid,\n",
    "                                  channel_user_id,\n",
    "                                  channel_user_name,\n",
    "                                  title=None, max_page_size=10000, max_title_size=100,\n",
    "                                  text_extractors_map=None,\n",
    "                                  upload_file=False):\n",
    "        file_doc_channel_uuid = self.get_file_documents_channel()['uuid']\n",
    "        file_object_ids = []\n",
    "        if upload_file:\n",
    "            file_object_id = self.upload_file(file_path)\n",
    "            file_object_ids = [file_object_id]\n",
    "        mt, _ = TextFindClient.__mimetype(file_path)\n",
    "        if text_extractors_map is None:\n",
    "            if mt == \"application/pdf\":\n",
    "                text_extractor = PdfExtractor(file_path, text_chunk_max_size=max_page_size)\n",
    "            elif mt == \"text/plain\":\n",
    "                text_extractor = TxtExtractor(file_path, text_chunk_max_size=max_page_size) \n",
    "            else:\n",
    "                raise Exception(f\"There is no default text extractor with the [{mt}] extension\"\n",
    "                                f\" for the provided file {file_path}\")\n",
    "        else:\n",
    "            raise Exception(f\"Please provide a text extractor for files with the {mt} extension.\")\n",
    "        text_arr = text_extractor.extract_text_chunks()\n",
    "        if title is None and len(text_arr[0]) > 0:\n",
    "            first_text_line = text_arr[0]\n",
    "            if len(first_text_line) > max_title_size:\n",
    "                title = first_text_line[0: max_title_size]\n",
    "            else:\n",
    "                title = first_text_line\n",
    "\n",
    "        file_document_uuid = str(uuid.uuid1())\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M\")\n",
    "        post_date = now.strftime(\"%Y-%d-%m\")\n",
    "\n",
    "        self.create_document(topic_uuid=topic_uuid,\n",
    "                             channel_uuid=file_doc_channel_uuid,\n",
    "                             document={\n",
    "                                 \"channel_user_id\": channel_user_id,\n",
    "                                 \"channel_user_name\": channel_user_name,\n",
    "                                 \"file_document_uuid\": file_document_uuid,\n",
    "                                 \"title\": f\"{title}\",\n",
    "                                 \"text\": f\"title: {title}\\n\"\n",
    "                                         f\"This is a file document with file_document_uuid equals to \"\n",
    "                                         f\"{file_document_uuid}\\n\",\n",
    "                                 \"url\": \"n/a\",\n",
    "                                 \"post_time\": current_time,\n",
    "                                 \"post_date\": post_date,\n",
    "                             },\n",
    "                             shared_with_group_uuids=[],\n",
    "                             file_object_ids=file_object_ids)\n",
    "        self.create_file_page_document(channel_user_id, channel_user_name, current_time, file_document_uuid,\n",
    "                                       post_date, text_arr, title, topic_uuid)\n",
    "\n",
    "    def create_file_page_document(self, channel_user_id, channel_user_name, current_time, file_document_uuid,\n",
    "                                  post_date, text_arr, title, topic_uuid):\n",
    "        file_page_channel_uuid = self.get_file_page_documents_channel()['uuid']\n",
    "        page_number = 1\n",
    "        for text in text_arr:\n",
    "            self.create_document(topic_uuid=topic_uuid,\n",
    "                                 channel_uuid=file_page_channel_uuid,\n",
    "                                 document={\n",
    "                                     \"channel_user_id\": channel_user_id,\n",
    "                                     \"channel_user_name\": channel_user_name,\n",
    "                                     \"file_document_uuid\": file_document_uuid,\n",
    "                                     \"title\": f\"{title} - page {page_number}\",\n",
    "                                     \"text\": text,\n",
    "                                     \"url\": \"n/a\",\n",
    "                                     \"post_time\": current_time,\n",
    "                                     \"post_date\": post_date,\n",
    "                                     \"page_number\": str(page_number)\n",
    "                                 },\n",
    "                                 shared_with_group_uuids=[],\n",
    "                                 file_object_ids=[])\n",
    "            page_number = page_number + 1\n",
    "\n",
    "    def get_file_documents_channel(self):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/metadata/channels',\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        channels = json.loads(get_request.text)\n",
    "        channels = list(filter(lambda channel: (channel['name'] == self.FILE_DOCUMENT_CHANNEL_NAME), channels))\n",
    "        return channels[0]\n",
    "\n",
    "    def get_file_page_documents_channel(self):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/metadata/channels',\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        channels = json.loads(get_request.text)\n",
    "        channels = list(filter(lambda channel: (channel['name'] == self.FILE_PAGE_DOCUMENTS_CHANNEL_NAME), channels))\n",
    "        return channels[0]\n",
    "\n",
    "    def search_semantic(self, q):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/semantic-search', params={'q': q},\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        searh_results = json.loads(get_request.text)\n",
    "        return searh_results[\"response\"]\n",
    "\n",
    "    def qa(self, q):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/qa', params={'q': q},\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        searh_results = json.loads(get_request.text)\n",
    "        return searh_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f7db3fd-99ed-45c2-9bf8-214ccc50edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://192.168.1.48:31854'\n",
    "api_key = '6arMPG04MzuFq0U9HNvh0uTXIjF5ANPl'\n",
    "files_to_import_dir = '/home/nicolae/caralislabs/import_files_to_textfind/text_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91aabeda-271d-444e-bf3d-f184bdfffbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['war_and_peace.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "listdir(f\"{files_to_import_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa7785fb-40d7-4422-bfd9-e89eb1552ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of TextFind client\n",
    "\n",
    "text_find_client = TextFindClient(url=url, app_key=api_key, verify_ssl_cert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b31a281c-a1f8-4be0-b769-03abbf1adf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uuid': 'b6f5b402-f3d0-4331-8f9f-b48538747583',\n",
       " 'name': '__file_documents__',\n",
       " 'description': '__file_documents__',\n",
       " 'keysValuesAggregatorGroovyScript': None,\n",
       " 'domExtractorScript': None,\n",
       " 'baseUrl': None,\n",
       " 'domain': None,\n",
       " 'channelPropertyDto': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_find_client.get_file_documents_channel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "170f27b4-6c2e-492a-b6cf-08fdeca3acd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': ['“Buonaparte?”'],\n",
       "  'TEXT_DATA_UUID': ['949a1bb1-075a-454b-9939-262f76bfdd3f'],\n",
       "  'id': '020e1b99-312b-4915-b6ac-947fc6a09ca2',\n",
       "  'score': 0.5965002},\n",
       " {'sentence': ['You will\\nbe surprised to hear that the reason for this is Buonaparte!'],\n",
       "  'TEXT_DATA_UUID': ['e9f61912-53a9-4b5d-a3f6-9d84536c1cb0'],\n",
       "  'id': '4d819ff2-7679-45e6-9b22-bc18f887b501',\n",
       "  'score': 0.5838278},\n",
       " {'sentence': ['You will\\nbe surprised to hear that the reason for this is Buonaparte!'],\n",
       "  'TEXT_DATA_UUID': ['e9f61912-53a9-4b5d-a3f6-9d84536c1cb0'],\n",
       "  'id': 'eb59b107-3b58-4f59-9443-d4b548f506ad',\n",
       "  'score': 0.5838278},\n",
       " {'sentence': ['When\\nmischief-makers don’t meddle even a German beats Buonaparte.'],\n",
       "  'TEXT_DATA_UUID': ['36d5b6b9-cee5-4d86-9891-5e1140152bbe'],\n",
       "  'id': '118d82ed-9bfe-42e0-a430-1bb59f46c0f2',\n",
       "  'score': 0.5616889},\n",
       " {'sentence': ['When\\nmischief-makers don’t meddle even a German beats Buonaparte.'],\n",
       "  'TEXT_DATA_UUID': ['36d5b6b9-cee5-4d86-9891-5e1140152bbe'],\n",
       "  'id': 'a56a637e-fd78-4271-b32d-afd85db44878',\n",
       "  'score': 0.5616889},\n",
       " {'sentence': ['“Buonaparte himself!...'],\n",
       "  'TEXT_DATA_UUID': ['b0e632e5-c013-451d-8205-8e8073f9d10c'],\n",
       "  'id': '5d87f240-16e9-4fe4-a3e6-f68c37de30aa',\n",
       "  'score': 0.5544114},\n",
       " {'sentence': ['And he says Buonaparte is in Braunau!'],\n",
       "  'TEXT_DATA_UUID': ['b0e632e5-c013-451d-8205-8e8073f9d10c'],\n",
       "  'id': '3c1713ea-ae33-4c2b-974f-22d4affafdf3',\n",
       "  'score': 0.54019296},\n",
       " {'sentence': ['“Buonaparte was born with a silver spoon in his mouth.'],\n",
       "  'TEXT_DATA_UUID': ['5e39669f-7c3a-4813-90af-161e4bb2649d'],\n",
       "  'id': '53e7f7d1-2580-4adf-942e-f4c7a59031e8',\n",
       "  'score': 0.51898336},\n",
       " {'sentence': ['“Well, then go off to your Buonaparte!'],\n",
       "  'TEXT_DATA_UUID': ['5e39669f-7c3a-4813-90af-161e4bb2649d'],\n",
       "  'id': '6204503e-f86c-40e6-9e67-4defe3e8366d',\n",
       "  'score': 0.5182507},\n",
       " {'sentence': ['Everybody said that Buonaparte himself was at Braunau.”'],\n",
       "  'TEXT_DATA_UUID': ['b0e632e5-c013-451d-8205-8e8073f9d10c'],\n",
       "  'id': 'a74cd43b-7c68-411c-a3d4-08838fe4ca25',\n",
       "  'score': 0.5167417}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute semantic search\n",
    "\n",
    "resulst = text_find_client.search_semantic(q='Buonaparte and German')\n",
    "resulst['docs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0581bb1c-af27-4536-aa0c-b798ac5ecff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer:he did not\\nwish to dress'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute question answering\n",
    "\n",
    "resulst = text_find_client.qa(q='what did Louise do when Andrúsha wake up?')\n",
    "'Answer:' + resulst['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7163b924-c1a1-4c84-a3b2-ee987a77e576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Today when Andrúsha (her eldest boy) woke up he did not\\nwish to dress and Mademoiselle Louise sent for me.',\n",
       "  'TEXT_DATA_UUID': ['d6cede08-f848-401c-86ab-d8845d80ba7e']},\n",
       " {'sentence': 'Today when Andrúsha (her eldest boy) woke up he did not\\nwish to dress and Mademoiselle Louise sent for me.',\n",
       "  'TEXT_DATA_UUID': ['d6cede08-f848-401c-86ab-d8845d80ba7e']},\n",
       " {'sentence': 'She woke late.',\n",
       "  'TEXT_DATA_UUID': ['127fbf1d-6abf-4247-b823-8f0b1b9d2c7a']},\n",
       " {'sentence': 'She woke late.',\n",
       "  'TEXT_DATA_UUID': ['127fbf1d-6abf-4247-b823-8f0b1b9d2c7a']},\n",
       " {'sentence': 'She did not sleep and did not leave her mother.',\n",
       "  'TEXT_DATA_UUID': ['cb23bbac-939e-465a-b7eb-a9dfa06c609d']},\n",
       " {'sentence': 'She did not sleep and did not leave her mother.',\n",
       "  'TEXT_DATA_UUID': ['cb23bbac-939e-465a-b7eb-a9dfa06c609d']},\n",
       " {'sentence': 'It was a long time before she could sleep.',\n",
       "  'TEXT_DATA_UUID': ['6830ecb8-4f8a-4a26-920c-aa1e8963a2a2']},\n",
       " {'sentence': 'After swallowing a little she had been so frightened that she\\nwoke Sónya and told her what she had done.',\n",
       "  'TEXT_DATA_UUID': ['49d30063-f976-411b-96c0-d7f6a1031221']},\n",
       " {'sentence': 'After swallowing a little she had been so frightened that she\\nwoke Sónya and told her what she had done.',\n",
       "  'TEXT_DATA_UUID': ['49d30063-f976-411b-96c0-d7f6a1031221']},\n",
       " {'sentence': 'She knew how Nicholas disliked being waked.',\n",
       "  'TEXT_DATA_UUID': ['6d47e30b-adad-49cb-be59-6a8a7cbb4be0']}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulst['sentence_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb217727-60f9-4f45-8d86-2dc67f7ff1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> start importing [war_and_peace.txt]\n",
      "> importing finished [war_and_peace.txt]\n"
     ]
    }
   ],
   "source": [
    "# Import all the files in TextFind\n",
    "\n",
    "for file_name in listdir(f\"{files_to_import_dir}\"):\n",
    "    print(f\"> start importing [{file_name}]\")\n",
    "    text_find_client.create_document_from_file(\n",
    "        channel_user_id=\"admin01\",\n",
    "        channel_user_name=\"admin01\",\n",
    "        title=f\"{file_name}\",\n",
    "        file_path=f\"{pdf_files_dir}/{file_name}\",\n",
    "        topic_uuid='62f0d33f-b2b5-4e65-a918-33cf152dfae3',\n",
    "        max_page_size=10000,\n",
    "        upload_file=True)\n",
    "    print(f\"> importing finished [{file_name}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb52d6-f5da-46d4-9e10-9b45abec8d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
