{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f91033-ae53-4016-9e4c-1eda3cdea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextFind demo on importing pdf documents and doing semantic searches and question answering\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf9961-1bc3-4897-9ccb-5bcc566bccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic class used by all the extractors to accomodate the extracted text for saving into TextFind\n",
    "\n",
    "import abc\n",
    "\n",
    "\n",
    "class BaseExtractor:\n",
    "    def __init__(self, text_chunk_max_size):\n",
    "        self.text_chunk_max_size = text_chunk_max_size\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def page_text_provider(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def extract_text(self, text_provider):\n",
    "        pass\n",
    "\n",
    "    def extract_text_chunks(self):\n",
    "        text_chunks_arr = []\n",
    "        current_text_chunk = ''\n",
    "        for text_provider in self.page_text_provider():\n",
    "            text_gen = self.extract_text(text_provider)\n",
    "            current_text_chunk += text_gen\n",
    "            if len(current_text_chunk) > self.text_chunk_max_size:\n",
    "                text_chunks_arr.append(current_text_chunk)\n",
    "                current_text_chunk = ''\n",
    "            else:\n",
    "                current_text_chunk += text_gen\n",
    "        if len(current_text_chunk) > 0:\n",
    "            text_chunks_arr.append(current_text_chunk)\n",
    "        return text_chunks_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62603970-8d1e-4ea5-819f-9c42d85b6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Pdf text extractor. Not that fancy but it does its job\n",
    "\n",
    "from pdfminer.layout import LTFigure\n",
    "from pdfminer.layout import LTTextBox\n",
    "from pdfminer.layout import LTChar\n",
    "from pdfminer.layout import LTTextLine\n",
    "from pdfminer.high_level import extract_pages\n",
    "\n",
    "\n",
    "class PdfExtractor(BaseExtractor):\n",
    "\n",
    "    def page_text_provider(self):\n",
    "        return extract_pages(self.pdf_file)\n",
    "\n",
    "    def extract_text(self, layout):\n",
    "        layout_text = ''\n",
    "        for lobj in layout:\n",
    "            layout_text += PdfExtractor.__text(lobj)\n",
    "        return layout_text\n",
    "\n",
    "    def __init__(self, pdf_file, text_chunk_max_size):\n",
    "        BaseExtractor.__init__(self, text_chunk_max_size=text_chunk_max_size)\n",
    "        self.pdf_file = pdf_file\n",
    "\n",
    "    def page_text_provider(self):\n",
    "        return extract_pages(self.pdf_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def __text(lobj):\n",
    "        lobj_text = ''\n",
    "        if isinstance(lobj, LTTextBox):\n",
    "            for element in lobj:\n",
    "                if isinstance(element, LTTextLine):\n",
    "                    lobj_text = lobj_text + element.get_text()\n",
    "        elif isinstance(lobj, LTFigure):\n",
    "            for element in lobj:\n",
    "                if isinstance(element, LTChar):\n",
    "                    lobj_text = lobj_text + element.get_text()\n",
    "        return lobj_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541341d4-b3a5-4907-8dc8-d9dfe0a65d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utitlity method to reject lines smaller than the min_line_length value.\n",
    "# and to remove some newlines (prone to changes, but for now it does the work)\n",
    "\n",
    "def text_chunk_arr_to_sentences(text_chunk_arr, min_line_length=10):\n",
    "    sentences = []\n",
    "    for text_chunk in text_chunk_arr:\n",
    "        lines = text_chunk.splitlines()\n",
    "        sentence = ''\n",
    "        for line in lines:\n",
    "            if line.endswith('.'):\n",
    "                sentence += line\n",
    "                sentences.append(sentence)\n",
    "                sentence = ''\n",
    "            else:\n",
    "                if len(line) >= min_line_length:\n",
    "                    sentence += line\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59f4c8-94c8-485b-94a6-a53ab809846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a minimal TextFind client to exercise a few TextFind endpoints like:\n",
    "# creating documents, uploading files, querying some basic metadata (channel and channel properties)\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import mimetypes\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TextFindClient:\n",
    "\n",
    "    API_KEY = \"api-key\"\n",
    "\n",
    "    FILE_DOCUMENT_CHANNEL_NAME = \"__file_documents__\"\n",
    "\n",
    "    FILE_PAGE_DOCUMENTS_CHANNEL_NAME = \"__file_page_documents__\"\n",
    "\n",
    "    def __init__(self, url, app_key=None, verify_ssl_cert=True):\n",
    "        self.app_key = app_key\n",
    "        self.url = url\n",
    "        self.verify_ssl_cert = verify_ssl_cert\n",
    "\n",
    "    def create_document(self, topic_uuid, channel_uuid, document, shared_with_group_uuids=[], file_object_ids=[]):\n",
    "        document_context = copy.deepcopy(document)\n",
    "        if len(file_object_ids) > 0:\n",
    "            document_context[\"relatedFileObjectIDs\"] = ','.join(file_object_ids)\n",
    "        else:\n",
    "            document_context[\"relatedFileObjectIDs\"] = None\n",
    "        document = dict()\n",
    "        document['topicUuid'] = topic_uuid\n",
    "        document['channelUuid'] = channel_uuid\n",
    "        document['sharedWithGroupUuids'] = shared_with_group_uuids\n",
    "        document['context'] = document_context\n",
    "        response = requests.post(url=f'{self.url}/sas-api/documents/text-documents',\n",
    "                                 json=document, headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        if not response.ok:\n",
    "            logging.error(response.text)\n",
    "            raise Exception(response.text)\n",
    "\n",
    "    @staticmethod\n",
    "    def __mimetype(file_path):\n",
    "        return mimetypes.guess_type(file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def __basename(file_path):\n",
    "        return os.path.basename(file_path)\n",
    "\n",
    "    def upload_file(self, file_path):\n",
    "        basename = TextFindClient.__basename(file_path)\n",
    "        mt = TextFindClient.__mimetype(file_path)\n",
    "\n",
    "        files = {'file': (basename, open(file_path, 'rb'), mt)}\n",
    "        response = requests.post(f'{self.url}/sas-api/file-manager/file',\n",
    "                                 files=files,\n",
    "                                 headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        if not response.ok:\n",
    "            logging.error(response.text)\n",
    "            raise Exception(response.text)\n",
    "        else:\n",
    "            is_success = json.loads(response.text)[\"success\"]\n",
    "            if is_success:\n",
    "                return json.loads(response.text)[\"fileObjectId\"]\n",
    "            else:\n",
    "                logging.debug(json.loads(response.text)[\"errorMessage\"])\n",
    "                raise Exception(json.loads(response.text)[\"errorMessage\"])\n",
    "\n",
    "    def create_document_from_file(self,\n",
    "                                  file_path,\n",
    "                                  topic_uuid,\n",
    "                                  channel_user_id,\n",
    "                                  channel_user_name,\n",
    "                                  title=None, max_page_size=10000, max_title_size=100,\n",
    "                                  text_extractors_map=None,\n",
    "                                  upload_file=False):\n",
    "        file_doc_channel_uuid = self.get_file_documents_channel()['uuid']\n",
    "        file_object_ids = []\n",
    "        if upload_file:\n",
    "            file_object_id = self.upload_file(file_path)\n",
    "            file_object_ids = [file_object_id]\n",
    "        mt, _ = TextFindClient.__mimetype(file_path)\n",
    "        if text_extractors_map is None:\n",
    "            if mt == \"application/pdf\":\n",
    "                pdf_text_extractor = PdfExtractor(file_path, text_chunk_max_size=max_page_size)\n",
    "            else:\n",
    "                raise Exception(f\"There is no default text extractor with the [{mt}] extension\"\n",
    "                                f\" for the provided file {file_path}\")\n",
    "        else:\n",
    "            raise Exception(f\"Please provide a text extractor for files with the {mt} extension.\")\n",
    "        text_arr = pdf_text_extractor.extract_text_chunks()\n",
    "        if title is None and len(text_arr[0]) > 0:\n",
    "            first_text_line = text_arr[0]\n",
    "            if len(first_text_line) > max_title_size:\n",
    "                title = first_text_line[0: max_title_size]\n",
    "            else:\n",
    "                title = first_text_line\n",
    "\n",
    "        file_document_uuid = str(uuid.uuid1())\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M\")\n",
    "        post_date = now.strftime(\"%Y-%d-%m\")\n",
    "\n",
    "        self.create_document(topic_uuid=topic_uuid,\n",
    "                             channel_uuid=file_doc_channel_uuid,\n",
    "                             document={\n",
    "                                 \"channel_user_id\": channel_user_id,\n",
    "                                 \"channel_user_name\": channel_user_name,\n",
    "                                 \"file_document_uuid\": file_document_uuid,\n",
    "                                 \"title\": f\"{title}\",\n",
    "                                 \"text\": f\"title: {title}\\n\"\n",
    "                                         f\"This is a file document with file_document_uuid equals to \"\n",
    "                                         f\"{file_document_uuid}\\n\",\n",
    "                                 \"url\": \"n/a\",\n",
    "                                 \"post_time\": current_time,\n",
    "                                 \"post_date\": post_date,\n",
    "                             },\n",
    "                             shared_with_group_uuids=[],\n",
    "                             file_object_ids=file_object_ids)\n",
    "        self.create_file_page_document(channel_user_id, channel_user_name, current_time, file_document_uuid,\n",
    "                                       post_date, text_arr, title, topic_uuid)\n",
    "\n",
    "    def create_file_page_document(self, channel_user_id, channel_user_name, current_time, file_document_uuid,\n",
    "                                  post_date, text_arr, title, topic_uuid):\n",
    "        file_page_channel_uuid = self.get_file_page_documents_channel()['uuid']\n",
    "        page_number = 1\n",
    "        for text in text_arr:\n",
    "            self.create_document(topic_uuid=topic_uuid,\n",
    "                                 channel_uuid=file_page_channel_uuid,\n",
    "                                 document={\n",
    "                                     \"channel_user_id\": channel_user_id,\n",
    "                                     \"channel_user_name\": channel_user_name,\n",
    "                                     \"file_document_uuid\": file_document_uuid,\n",
    "                                     \"title\": f\"{title} - page {page_number}\",\n",
    "                                     \"text\": text,\n",
    "                                     \"url\": \"n/a\",\n",
    "                                     \"post_time\": current_time,\n",
    "                                     \"post_date\": post_date,\n",
    "                                     \"page_number\": str(page_number)\n",
    "                                 },\n",
    "                                 shared_with_group_uuids=[],\n",
    "                                 file_object_ids=[])\n",
    "            page_number = page_number + 1\n",
    "\n",
    "    def get_file_documents_channel(self):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/metadata/channels',\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        channels = json.loads(get_request.text)\n",
    "        channels = list(filter(lambda channel: (channel['name'] == self.FILE_DOCUMENT_CHANNEL_NAME), channels))\n",
    "        return channels[0]\n",
    "\n",
    "    def get_file_page_documents_channel(self):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/metadata/channels',\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        channels = json.loads(get_request.text)\n",
    "        channels = list(filter(lambda channel: (channel['name'] == self.FILE_PAGE_DOCUMENTS_CHANNEL_NAME), channels))\n",
    "        return channels[0]\n",
    "\n",
    "    def search_semantic(self, q):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/semantic-search', params={'q': q},\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        searh_results = json.loads(get_request.text)\n",
    "        return searh_results[\"response\"]\n",
    "\n",
    "    def qa(self, q):\n",
    "        get_request = requests.get(f'{self.url}/sas-api/qa', params={'q': q},\n",
    "                                   headers={self.API_KEY: self.app_key}, verify=self.verify_ssl_cert)\n",
    "        searh_results = json.loads(get_request.text)\n",
    "        return searh_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7db3fd-99ed-45c2-9bf8-214ccc50edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://192.168.1.48:31854'\n",
    "api_key = '6arMPG04MzuFq0U9HNvh0uTXIjF5ANPl'\n",
    "pdf_files_dir = '/home/nicolae/caralislabs/import_files_to_textfind/pdf_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aabeda-271d-444e-bf3d-f184bdfffbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "listdir(f\"{pdf_files_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7785fb-40d7-4422-bfd9-e89eb1552ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of TextFind client\n",
    "\n",
    "text_find_client = TextFindClient(url=url, app_key=api_key, verify_ssl_cert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a281c-a1f8-4be0-b769-03abbf1adf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_find_client.get_file_documents_channel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f27b4-6c2e-492a-b6cf-08fdeca3acd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute semantic search\n",
    "\n",
    "resulst = text_find_client.search_semantic(q='Biden administration has been pushing EVs')\n",
    "resulst['docs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581bb1c-af27-4536-aa0c-b798ac5ecff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute question answering\n",
    "\n",
    "resulst = text_find_client.qa(q='What did Toyota Chairman say with regards to Electric Cars')\n",
    "'Answer:' + resulst['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163b924-c1a1-4c84-a3b2-ee987a77e576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resulst['sentence_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb217727-60f9-4f45-8d86-2dc67f7ff1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the files in TextFind\n",
    "\n",
    "for file_name in listdir(f\"{pdf_files_dir}\"):\n",
    "    print(f\"> start importing [{file_name}]\")\n",
    "    text_find_client.create_document_from_file(\n",
    "        channel_user_id=\"admin01\",\n",
    "        channel_user_name=\"admin01\",\n",
    "        title=f\"{file_name}\",\n",
    "        file_path=f\"{pdf_files_dir}/{file_name}\",\n",
    "        topic_uuid='62f0d33f-b2b5-4e65-a918-33cf152dfae3',\n",
    "        max_page_size=10000,\n",
    "        upload_file=True)\n",
    "    print(f\"> importing finished [{file_name}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb52d6-f5da-46d4-9e10-9b45abec8d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
